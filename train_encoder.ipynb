{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-07 13:57:51.666684: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-09-07 13:57:51.669970: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/kiliakis/install/lib:/usr/lib/x86_64-linux-gnu\n",
      "2022-09-07 13:57:51.669980: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# Train the ML model\n",
    "\n",
    "from models import extendedCED, mse_loss_encoder, mse_loss_decoder\n",
    "from utils import load_model_data_new, unnormalize_params, assess_decoder\n",
    "from utils import plot_loss, normalize_params, load_encoder_data\n",
    "import time\n",
    "import glob\n",
    "from telnetlib import EC\n",
    "import tensorflow as tf\n",
    "import yaml\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "data_dir = '/eos/kiliakis/tomo_data/datasets'\n",
    "timestamp = datetime.now().strftime(\"%Y_%m_%d_%H-%M-%S\")\n",
    "\n",
    "# Data specific\n",
    "IMG_OUTPUT_SIZE = 128\n",
    "BUFFER_SIZE = 1000\n",
    "BATCH_SIZE = 32  # 8\n",
    "latent_dim = 7  # 6 + the new VrfSPS\n",
    "additional_latent_dim = 1\n",
    "\n",
    "# Train specific\n",
    "models_to_train = ['encoder']\n",
    "train_cfg = {\n",
    "    'encoder': {\n",
    "        'epochs': 2,\n",
    "        'lr': 2e-4,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Keep only a small percentage of the entire dataset\n",
    "# for faster testing.\n",
    "dataset_keep_percent = 0.1\n",
    "# cnn_filters = [32, 64, 128, 256, 512, 1024]\n",
    "cnn_filters = [32]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize directories\n",
    "trial_dir = os.path.join('./trials/', timestamp)\n",
    "weights_dir = os.path.join(trial_dir, 'weights')\n",
    "plots_dir = os.path.join(trial_dir, 'plots')\n",
    "\n",
    "# Initialize train/ test / validation paths\n",
    "ML_dir = os.path.join(data_dir, 'ML_data')\n",
    "TRAINING_PATH = os.path.join(ML_dir, 'TRAINING')\n",
    "VALIDATION_PATH = os.path.join(ML_dir, 'VALIDATION')\n",
    "# TESTING_PATH = os.path.join(ML_dir, 'TESTING')\n",
    "assert os.path.exists(TRAINING_PATH)\n",
    "assert os.path.exists(VALIDATION_PATH)\n",
    "# assert os.path.exists(TESTING_PATH)\n",
    "\n",
    "# create the directory to store the results\n",
    "os.makedirs(trial_dir, exist_ok=True)\n",
    "os.makedirs(weights_dir, exist_ok=False)\n",
    "os.makedirs(plots_dir, exist_ok=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2920153100.py, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_646148/2920153100.py\"\u001b[0;36m, line \u001b[0;32m20\u001b[0m\n\u001b[0;31m    train_dataset = train_dataset.map(\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Create the datasets\n",
    "files = glob.glob(TRAINING_PATH + '/*.pk')\n",
    "files = files[:int(len(files) * dataset_keep_percent)]\n",
    "# train_dataset = tf.data.Dataset.list_files(TRAINING_PATH + '/*.pk')\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(files)\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "# train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "train_dataset = train_dataset.map(lambda x: tf.py_function(load_encoder_data, [x], (tf.float32,\n",
    "                                                                                    [tf.float32, tf.float32, tf.float32, tf.float32,\n",
    "                                                                                     tf.float32, tf.float32, tf.float32]))\n",
    "\n",
    "# train_dataset = train_dataset.map(\n",
    "#     lambda x: tf.py_function(load_model_data_new,\n",
    "                            #  [x],\n",
    "                            #  [tf.float32, tf.float32, tf.float32, tf.string,\n",
    "                            #   tf.float32, tf.float32, tf.float32, tf.float32,\n",
    "                            #   tf.float32, tf.float32, tf.float32, tf.float32,\n",
    "                            #   tf.float32]))\n",
    "# train_dataset = train_dataset.map(lambda *x: x[1])\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda *x: (x[1], tf.convert_to_tensor(x[4:11])))\n",
    "# y_train = train_dataset.map(lambda *x: normalize_params(*x[4:11]))\n",
    "\n",
    "files = glob.glob(VALIDATION_PATH + '/*.pk')\n",
    "files = files[:int(len(files) * dataset_keep_percent)]\n",
    "# valid_dataset = tf.data.Dataset.list_files(VALIDATION_PATH + '/*.pk')\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices(files)\n",
    "valid_dataset = valid_dataset.shuffle(BUFFER_SIZE)\n",
    "# valid_dataset = valid_dataset.batch(BATCH_SIZE)\n",
    "# valid_dataset = valid_dataset.map(lambda x: tf.py_function(load_model_data_new, [x],\n",
    "valid_dataset = valid_dataset.map(\n",
    "    lambda x: tf.py_function(load_encoder_data, [x], (tf.float32,\n",
    "    [tf.float32, tf.float32, tf.float32, tf.float32,\n",
    "     tf.float32, tf.float32, tf.float32]))\n",
    "\n",
    "# valid_dataset=valid_dataset.map(lambda x: tf.py_function(load_encoder_data, [x],\n",
    "#                                                            [tf.float32, tf.float32, tf.float32, tf.string,\n",
    "#                                                             tf.float32, tf.float32, tf.float32, tf.float32,\n",
    "#                                                             tf.float32, tf.float32, tf.float32, tf.float32,\n",
    "#                                                             tf.float32]))\n",
    "# valid_dataset = valid_dataset.map(lambda *x: (x[1], tf.convert_to_tensor(x[4:11])))\n",
    "# y_valid = valid_dataset.map(lambda *x: normalize_params(*x[4:11]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 6.00e+00 -1.40e+01  1.36e-09  5.70e+10  7.90e+00  4.10e+00  8.30e+00], shape=(7,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for features, targets in valid_dataset:\n",
    "    print(targets)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 63, 63, 32)        320       \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 127008)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 7)                 889063    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 889,383\n",
      "Trainable params: 889,383\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Model instantiation\n",
    "input_shape = (IMG_OUTPUT_SIZE, IMG_OUTPUT_SIZE, 1)\n",
    "\n",
    "eCED = extendedCED(latent_dim, additional_latent_dim, input_shape,\n",
    "                   filters=cnn_filters)\n",
    "\n",
    "print(eCED.encoder.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "The Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Train the encoder\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(train_cfg['encoder']['lr'])\n",
    "\n",
    "eCED.encoder.compile(optimizer=optimizer, loss='mse')\n",
    "history = eCED.encoder.fit(\n",
    "    train_dataset, epochs=train_cfg['encoder']['epochs'],\n",
    "    validation_data=valid_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "79ab8fd01a8cec42884b8b2a5d7fb4751c5402d97e9e61d151ed5c6a6352873c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

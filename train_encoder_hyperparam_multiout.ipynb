{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the ML model\n",
    "from utils import sample_files, encoder_files_to_tensors\n",
    "from utils import plot_loss, load_encoder_data\n",
    "from models import EncoderSingle\n",
    "\n",
    "import time\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "# from tensorboard.plugins.hparams import api as hp\n",
    "from tensorflow import keras\n",
    "import yaml\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_Turns_Case = 1\n",
    "var_names = ['phEr', 'enEr', 'bl',\n",
    "             'inten', 'Vrf', 'mu', 'VrfSPS']\n",
    "# Initialize parameters\n",
    "data_dir = '/eos/user/k/kiliakis/tomo_data/datasets_encoder_02-12-22'\n",
    "# data_dir = './tomo_data/datasets'\n",
    "# timestamp = datetime.now().strftime(\"%Y_%m_%d_%H-%M-%S\")\n",
    "timestamp = 'encoder-1'\n",
    "print('Using timestamp: ', timestamp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize directories\n",
    "trial_dir = os.path.join('./trials/', timestamp)\n",
    "# weights_dir = os.path.join(trial_dir, 'weights')\n",
    "# plots_dir = os.path.join(trial_dir, 'plots')\n",
    "# logs_dir = os.path.join(trial_dir, 'logs')\n",
    "\n",
    "# Initialize train/ test / validation paths\n",
    "ML_dir = os.path.join(data_dir, 'ML_data')\n",
    "TRAINING_PATH = os.path.join(ML_dir, 'TRAINING')\n",
    "VALIDATION_PATH = os.path.join(ML_dir, 'VALIDATION')\n",
    "assert os.path.exists(TRAINING_PATH)\n",
    "assert os.path.exists(VALIDATION_PATH)\n",
    "\n",
    "# create the directory to store the results\n",
    "os.makedirs(trial_dir, exist_ok=True)\n",
    "# os.makedirs(weights_dir, exist_ok=False)\n",
    "# os.makedirs(plots_dir, exist_ok=False)\n",
    "# os.makedirs(logs_dir, exist_ok=False)\n",
    "\n",
    "# Initialize GPU\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "device_to_use = 0\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        tf.config.experimental.set_memory_growth(gpus[device_to_use], True)\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[device_to_use],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=12*1024)])\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(\n",
    "            logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print('No GPU available, using the CPU')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_OUTPUT_SIZE = 128\n",
    "BATCH_SIZE = 32  # 8\n",
    "input_shape = (IMG_OUTPUT_SIZE, IMG_OUTPUT_SIZE, 1)\n",
    "\n",
    "# Train specific\n",
    "train_cfg = {\n",
    "    'epochs': 10,\n",
    "    'dense_layers': [16],\n",
    "    'filters': [8],\n",
    "    'cropping': [0, 0],\n",
    "    'kernel_size': 7,\n",
    "    'strides': [2, 2],\n",
    "    'activation': 'relu',\n",
    "    'pooling': None,\n",
    "    'pooling_size': [0, 0],\n",
    "    'pooling_strides': [1, 1],\n",
    "    'pooling_padding': 'valid',\n",
    "    'dropout': 0.1,\n",
    "    'loss': 'mse',\n",
    "    'lr': 1e-3,\n",
    "    'dataset%': 1,\n",
    "    'normalization': 'minmax',\n",
    "    'loss_weights': [6],\n",
    "    'batch_size': 32\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_t = time.time()\n",
    "# Create the datasets\n",
    "# 1. Randomly select the training data\n",
    "file_names = sample_files(TRAINING_PATH, train_cfg['dataset%'], keep_every=num_Turns_Case)\n",
    "print('Number of Training files: ', len(file_names))\n",
    "\n",
    "x_train, y_train = encoder_files_to_tensors(\n",
    "    file_names, normalization=train_cfg['normalization'])\n",
    "\n",
    "# Repeat for validation data\n",
    "file_names = sample_files(\n",
    "    VALIDATION_PATH, train_cfg['dataset%'], keep_every=num_Turns_Case)\n",
    "print('Number of Validation files: ', len(file_names))\n",
    "\n",
    "x_valid, y_valid = encoder_files_to_tensors(\n",
    "    file_names, normalization=train_cfg['normalization'])\n",
    "\n",
    "end_t = time.time()\n",
    "print(\n",
    "    f'\\n---- Input files have been read, elapsed: {end_t - start_t} ----\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# plot some of the outputs\n",
    "\n",
    "nrows = 3\n",
    "# Get nrows * nrows random images\n",
    "sample = np.random.choice(np.arange(len(x_train)),\n",
    "                          size=nrows * nrows, replace=False)\n",
    "\n",
    "samples_X = tf.gather(x_train, sample)\n",
    "samples_y = tf.gather(y_train, sample)\n",
    "\n",
    "# Create 3x3 grid of figures\n",
    "fig, axes = plt.subplots(ncols=nrows, nrows=nrows, figsize=(12, 12))\n",
    "axes = np.ravel(axes)\n",
    "for i in range(len(axes)):\n",
    "    ax = axes[i]\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    # show the image\n",
    "    ax.imshow(samples_X[i], cmap='jet')\n",
    "    # Set the label\n",
    "    title = ','.join([f'{num:.1f}' for num in samples_y[i]])\n",
    "    ax.set_title(f'{title}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(timestamp)\n",
    "print(trial_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "IMG_OUTPUT_SIZE = 128\n",
    "BATCH_SIZE = 32  # 8\n",
    "input_shape = (IMG_OUTPUT_SIZE, IMG_OUTPUT_SIZE, 1)\n",
    "\n",
    "# Train specific\n",
    "train_cfg = {\n",
    "    'epochs': 10,\n",
    "    'dense_layers': [16],\n",
    "    'filters': [8],\n",
    "    'cropping': [0, 0],\n",
    "    'kernel_size': 7,\n",
    "    'strides': [2, 2],\n",
    "    'activation': 'relu',\n",
    "    'pooling': None,\n",
    "    'pooling_size': [0, 0],\n",
    "    'pooling_strides': [1, 1],\n",
    "    'pooling_padding': 'valid',\n",
    "    'dropout': 0.1,\n",
    "    'loss': 'mse',\n",
    "    'lr': 1e-3,\n",
    "    'dataset%': 1,\n",
    "    'normalization': 'minmax',\n",
    "    'loss_weights': [5],\n",
    "    'batch_size': 32\n",
    "}\n",
    "\n",
    "model_cfg = {\n",
    "    'mu': {\n",
    "        'epochs': 15,\n",
    "        'cropping': [6, 6],\n",
    "        'filters': [8, 16, 32],\n",
    "        'kernel_size': [13, 7, 3],\n",
    "        'strides': [2, 2],\n",
    "        'dense_layers': [1024, 256, 32],\n",
    "        'activation': 'relu',\n",
    "        'pooling': None,\n",
    "        'dropout': 0.0,\n",
    "        'lr': 1e-3,\n",
    "        'normalization': 'minmax',\n",
    "        'batch_size': 32\n",
    "    },\n",
    "}\n",
    "\n",
    "param_space = {\n",
    "    'cropping': [[0, 0]],\n",
    "    'kernel_size': [[3, 3, 3], [5, 5, 3], [5, 5, 5], [7, 5, 3], [7, 7, 7], [13, 7, 3]],\n",
    "    'filters': [[2, 4, 8], [4, 8, 16], [8, 16, 32], [4, 4, 8], [8, 8, 16]],\n",
    "    'dense_layers': [[1024, 512, 32], [1024, 512, 64], [1024, 512, 256],\n",
    "                     [1024, 256, 32], [1024, 256, 64], [1024, 256, 128],\n",
    "                     [2048, 1024, 64], [2048, 1024, 128], [2048, 1024, 256],\n",
    "                     [2048, 512, 32], [2048, 512, 64], [2048, 512, 128]]\n",
    "}\n",
    "\n",
    "\n",
    "def train_test_model(var_name, x_train, y_train, x_valid, y_valid, hparamdir, hparams):\n",
    "    cfg = train_cfg.copy()\n",
    "    cfg.update(model_cfg.get(var_name, {}))\n",
    "    cfg.update(hparams)\n",
    "    \n",
    "    model = EncoderSingle(input_shape=input_shape,\n",
    "                         output_name=var_name,\n",
    "                         **cfg)\n",
    "    weights_dir = os.path.join(hparamdir, 'weights')\n",
    "    # callbacks, save the best model, and early stop if no improvement in val_loss\n",
    "#     stop_early = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "#                                                patience=10, restore_best_weights=True)\n",
    "#     save_best = keras.callbacks.ModelCheckpoint(filepath=os.path.join(weights_dir, f'encoder_{var_name}.h5'),\n",
    "#                                                 monitor='val_loss', save_best_only=True)\n",
    "    callbacks = [\n",
    "#         save_best,\n",
    "        keras.callbacks.TensorBoard(hparamdir,\n",
    "#                                             histogram_freq=1,\n",
    "#                                             write_graph=True,\n",
    "#                                             write_images=True,\n",
    "                                            update_freq='epoch',\n",
    "#                                             embeddings_freq=1\n",
    "                                            ),\n",
    "#                  hp.KerasCallback(logdir, hparams)\n",
    "                ]\n",
    "    start_t = time.time()\n",
    "    history = model.model.fit(\n",
    "        x=x_train, y=y_train, \n",
    "        epochs=cfg['epochs'],\n",
    "        validation_data=(x_valid, y_valid),\n",
    "        callbacks=callbacks, \n",
    "        batch_size=cfg['batch_size'],\n",
    "        verbose=0)\n",
    "    total_t = time.time() - start_t\n",
    "    val_loss = model.model.evaluate(x_valid, y_valid)\n",
    "    \n",
    "    # save file with experiment configuration\n",
    "    config_dict = {}\n",
    "    config_dict['encoder'] = cfg.copy()\n",
    "    config_dict['encoder'].update({\n",
    "        'min_train_loss': float(np.min(history.history['loss'])),\n",
    "        'min_valid_loss': float(np.min(history.history['val_loss'])),\n",
    "        'total_train_time': total_t,\n",
    "        'used_gpus': len(gpus)\n",
    "    })\n",
    "\n",
    "    # save config_dict\n",
    "    with open(os.path.join(hparamdir, 'encoder-summary.yml'), 'w') as configfile:\n",
    "        yaml.dump(config_dict, configfile, default_flow_style=False)\n",
    "    \n",
    "    return history.history, val_loss\n",
    "\n",
    "var_name = 'mu'\n",
    "train = tf.gather(y_train, var_names.index(var_name), axis=1)\n",
    "valid = tf.gather(y_valid, var_names.index(var_name), axis=1)\n",
    "\n",
    "# runs = os.listdir(trial_dir)\n",
    "# runs = [int(r.split('-')[-1]) for r in runs]\n",
    "session_num = 0\n",
    "# historyMulti = []\n",
    "keys, values = zip(*param_space.items())\n",
    "total_runs = np.prod([len(v) for v in param_space.values()])\n",
    "\n",
    "overall_dict = {}\n",
    "for bundle in product(*values):\n",
    "    hparams = dict(zip(keys, bundle))\n",
    "    run_name = f\"run-{session_num}\"\n",
    "    print(f'--- Starting trial: {run_name}/{total_runs}')\n",
    "#     print({h.name: hparams[h] for h in hparams})\n",
    "    print(hparams)\n",
    "    start_t = time.time()\n",
    "    history, loss = train_test_model(var_name, x_train, train, x_valid, valid, os.path.join(trial_dir, run_name), hparams)\n",
    "    total_time = time.time() - start_t\n",
    "    train_loss = np.min(history[\"loss\"])\n",
    "    valid_loss = np.min(history[\"val_loss\"])\n",
    "    overall_dict[run_name] = {'time': total_time, 'train': train_loss, 'valid': valid_loss, 'history': history}\n",
    "    overall_dict[run_name].update(hparams)\n",
    "    print(f'---- Training complete, epochs: {len(history[\"loss\"])}, train loss {np.min(history[\"loss\"]):.2e}, valid loss {np.min(history[\"val_loss\"]):.2e}, total time {total_time} ----')\n",
    "    \n",
    "#     historyMulti.append(history)\n",
    "    session_num += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "timestamp = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "os.makedirs('hparam_dicts', exist_ok=True)\n",
    "fname = f'hparam_dicts/{var_name}_{timestamp}.pkl'\n",
    "with open(fname, 'wb') as handle:\n",
    "    pickle.dump(overall_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data have been restored, datapoints: 72\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "var_name = 'enEr'\n",
    "# load the data from the hparam dict\n",
    "all_data = {}\n",
    "idx = 0\n",
    "for fname in os.listdir('hparam_dicts'):\n",
    "    if ('.pkl' not in fname) or (var_name not in fname):\n",
    "        continue\n",
    "    with open('hparam_dicts/' + fname, 'rb') as handle:\n",
    "        temp_d = pickle.load(handle)\n",
    "    for k, v in temp_d.items():\n",
    "        all_data[idx] = v\n",
    "        idx+=1\n",
    "print(f'All data have been restored, datapoints: {len(all_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       train     valid  cropping kernel_size      filters      dense_layers\n",
      "31  0.000054  0.000032    [6, 6]   [3, 3, 3]  [8, 16, 32]  [1024, 256, 128]\n",
      "30  0.000054  0.000032    [6, 6]   [3, 3, 3]  [8, 16, 32]   [1024, 256, 64]\n",
      "25  0.000058  0.000034    [6, 6]   [3, 3, 3]   [4, 8, 16]  [1024, 512, 128]\n",
      "36  0.000064  0.000034    [6, 6]   [5, 5, 5]  [8, 16, 32]   [1024, 512, 64]\n",
      "4   0.000046  0.000035    [0, 0]   [3, 3, 3]  [8, 16, 32]   [1024, 512, 64]\n",
      "..       ...       ...       ...         ...          ...               ...\n",
      "15  0.000064  0.000065    [0, 0]   [5, 5, 5]  [8, 16, 32]  [1024, 256, 128]\n",
      "49  0.000068  0.000066  [12, 12]   [3, 3, 3]   [4, 8, 16]  [1024, 512, 128]\n",
      "64  0.000067  0.000067  [12, 12]   [7, 7, 7]   [4, 8, 16]   [1024, 512, 64]\n",
      "68  0.000079  0.000070  [12, 12]   [7, 7, 7]  [8, 16, 32]   [1024, 512, 64]\n",
      "0   0.000059  0.000083    [0, 0]   [3, 3, 3]   [4, 8, 16]   [1024, 512, 64]\n",
      "\n",
      "[72 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "best_params = pd.DataFrame.from_dict(all_data, orient='index')\n",
    "best_params.drop(columns=['history', 'time'], inplace=True)\n",
    "best_params.sort_values(by='valid', inplace=True)\n",
    "print(best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "def filter_dict(data, filters):\n",
    "    keep_data = {}\n",
    "    for k, v in data.items():\n",
    "        to_keep=True\n",
    "        for f, fv in filters.items():\n",
    "            to_keep &= v[f] in fv\n",
    "        if to_keep:\n",
    "            keep_data[k] = v\n",
    "    return keep_data\n",
    "\n",
    "filters = {'cropping': [[0,0]],\n",
    "           'kernel_size': [[5, 5, 3], [5, 5, 5], [7, 5, 3], [7, 7, 7]],\n",
    "           'filters': [[4, 8, 16], [8, 16, 32]]\n",
    "          }\n",
    "\n",
    "filtered_data = filter_dict(all_data, filters)\n",
    "\n",
    "\n",
    "hdf = pd.DataFrame.from_dict(filtered_data, orient='index')\n",
    "hdf.drop(columns=['time', 'train', 'history'], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig = px.parallel_categories(hdf,\n",
    "                             color='valid',\n",
    "                             dimensions=list(param_space.keys()),\n",
    "                             color_continuous_scale=px.colors.sequential.Bluered_r,\n",
    "                             color_continuous_midpoint=None,\n",
    "                            height=1000)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from itertools import product\n",
    "# import ray\n",
    "# from ray import air, tune\n",
    "# from ray.air import session\n",
    "# from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "# from ray.tune.integration.keras import TuneReportCallback\n",
    "# # import keras_tuner as kt\n",
    "# # from hyperopt import hp\n",
    "# from functools import partial\n",
    "\n",
    "# IMG_OUTPUT_SIZE = 128\n",
    "# BATCH_SIZE = 32  # 8\n",
    "# input_shape = (IMG_OUTPUT_SIZE, IMG_OUTPUT_SIZE, 1)\n",
    "\n",
    "# # Train specific\n",
    "# train_cfg = {\n",
    "#     'epochs': 10,\n",
    "#     'dense_layers': [16],\n",
    "#     'filters': [8],\n",
    "#     'cropping': [0, 0],\n",
    "#     'kernel_size': 7,\n",
    "#     'strides': [2, 2],\n",
    "#     'activation': 'relu',\n",
    "#     'pooling': None,\n",
    "#     'pooling_size': [0, 0],\n",
    "#     'pooling_strides': [1, 1],\n",
    "#     'pooling_padding': 'valid',\n",
    "#     'dropout': 0.1,\n",
    "#     'loss': 'mse',\n",
    "#     'lr': 1e-3,\n",
    "#     'dataset%': 1,\n",
    "#     'normalization': 'minmax',\n",
    "#     'loss_weights': [6],\n",
    "#     'batch_size': 32\n",
    "# }\n",
    "\n",
    "# model_cfg = {\n",
    "#     # best VrfSPS config --> 2.78e-03 val loss\n",
    "#     'VrfSPS': {\n",
    "#         'epochs': 100,\n",
    "#         'cropping': [(6, 6), (6, 64)],\n",
    "#         'filters': [4, 8, 16],\n",
    "#         'kernel_size': [(13, 3), (7, 3), 3],\n",
    "#         'strides': [2, 2],\n",
    "#         'dense_layers': [1024, 512, 32],\n",
    "#         'activation': 'relu',\n",
    "#         'loss': 'mse',\n",
    "#         'pooling': None,\n",
    "#         'dropout': 0.0,\n",
    "#         'lr': 1e-3,\n",
    "#         'normalization': 'minmax',\n",
    "#         'batch_size': 32\n",
    "#     },\n",
    "# }\n",
    "\n",
    "# param_space = {\n",
    "#     'dense_layers': tune.choice([[32], [64], [128], [256], [512], [1024]])\n",
    "# }\n",
    "\n",
    "# var_name = 'VrfSPS'\n",
    "# train = tf.gather(y_train, var_names.index(var_name), axis=1)\n",
    "# valid = tf.gather(y_valid, var_names.index(var_name), axis=1)\n",
    "\n",
    "# def train_test_model(var_name, x_train, y_train, x_valid, y_valid, hparams):\n",
    "#     cfg = train_cfg.copy()\n",
    "#     cfg.update(model_cfg.get(var_name, {}))\n",
    "#     cfg.update(hparams)\n",
    "#     model = EncoderSingle(input_shape=input_shape,\n",
    "#                          output_name=var_name,\n",
    "#                          **cfg)\n",
    "    \n",
    "#     # callbacks, save the best model, and early stop if no improvement in val_loss\n",
    "# #     stop_early = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "# #                                                patience=10, restore_best_weights=True)\n",
    "#     save_best = keras.callbacks.ModelCheckpoint(filepath=os.path.join(weights_dir, f'encoder_{var_name}.h5'),\n",
    "#                                                 monitor='val_loss', save_best_only=True)\n",
    "#     callbacks = [save_best,\n",
    "# #                  keras.callbacks.TensorBoard(logdir),\n",
    "# #                  hp.KerasCallback(logdir, hparams)\n",
    "#                 ]\n",
    "#     history = model.model.fit(\n",
    "#         x=x_train, y=y_train, \n",
    "#         epochs=cfg['epochs'],\n",
    "#         validation_data=(x_valid, y_valid),\n",
    "#         callbacks=callbacks, \n",
    "#         batch_size=cfg['batch_size'],\n",
    "#         verbose=0)\n",
    "    \n",
    "#     val_loss = model.model.evaluate(x_valid, y_valid)\n",
    "#     session.report({\"score\": val_loss})\n",
    "# #     return val_loss\n",
    "\n",
    "\n",
    "# tuner = tune.Tuner(\n",
    "#     partial(train_test_model, 'VrfSPS', x_train, train, x_valid, valid),\n",
    "#     param_space=param_space,\n",
    "#     tune_config=tune.TuneConfig(metric=\"score\", mode='min', num_samples=6)\n",
    "# )\n",
    "# results = tuner.fit()\n",
    "# print(\"Best hyperparameters found were: \", results.get_best_result().config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "train_loss_l = []\n",
    "valid_loss_l = []\n",
    "for k, v in historyMulti.items():\n",
    "    if 'val' in k:\n",
    "        valid_loss_l.append(v)\n",
    "    else:\n",
    "        train_loss_l.append(v)\n",
    "\n",
    "train_loss_l = np.mean(train_loss_l, axis=0)\n",
    "valid_loss_l = np.mean(valid_loss_l, axis=0)\n",
    "# print(train_loss_l)\n",
    "plot_loss({'training': train_loss_l, 'validation': valid_loss_l},\n",
    "          title='Encoder Train/Validation Loss')\n",
    "\n",
    "plot_loss(historyMulti, title='Encoder loss per output')\n",
    "\n",
    "# print(historyMulti)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save file with experiment configuration\n",
    "config_dict = {}\n",
    "config_dict['encoder'] = train_cfg.copy()\n",
    "config_dict['encoder'].update({\n",
    "    'min_train_loss': float(np.min(train_loss_l)),\n",
    "    'min_valid_loss': float(np.min(valid_loss_l)),\n",
    "    'total_train_time': total_time,\n",
    "    'used_gpus': len(gpus)\n",
    "})\n",
    "\n",
    "# save config_dict\n",
    "with open(os.path.join(trial_dir, 'encoder-summary.yml'), 'w') as configfile:\n",
    "    yaml.dump(config_dict, configfile, default_flow_style=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "79ab8fd01a8cec42884b8b2a5d7fb4751c5402d97e9e61d151ed5c6a6352873c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

- encoder:
    epochs: 100
    dataset%: 1
    activation: relu
    dropout: 0.0
    normalization: minmax
    img_normalize: 'off'
    pooling: null
    pooling_padding: valid
    pooling_size: [2, 2]
    pooling_strides: [1, 1]
    strides: [2, 2]
    conv_padding: same
    conv_batchnorm: false
    use_bias: false
    loss: mse
    loss_weights: [0, 1, 2, 4]
    lr: 1e-3
    batch_size: 32
  model_cfg:
    phEr:
      cropping: [0, 0]
      dense_layers: [1024, 512, 32]
      filters: [8, 16, 32]
      kernel_size: 7
      batchnorm: false
    enEr:
      cropping: [0, 0]
      dense_layers: [1024, 256, 64]
      filters: [8, 16, 32]
      kernel_size: 9
      batchnorm: false
    bl:
      cropping: [0, 0]
      dense_layers: [1024, 512, 64]
      filters: [16, 32, 64]
      kernel_size: 5
      batchnorm: false
    Vrf:
      cropping: [12, 12]
      dense_layers: [1024, 256, 64]
      filters: [8, 16, 32]
      kernel_size: 3
      batchnorm: false

- encoder:
    epochs: 100
    dataset%: 1
    activation: relu
    dropout: 0.0
    normalization: minmax
    img_normalize: 'off'
    pooling: null
    pooling_padding: valid
    pooling_size: [2, 2]
    pooling_strides: [1, 1]
    strides: [2, 2]
    conv_padding: same
    conv_batchnorm: false
    use_bias: false
    loss: mse
    loss_weights: [0, 1, 2, 4]
    lr: 1e-3
    batch_size: 32
  model_cfg:
    phEr:
      cropping: [0, 0]
      dense_layers: [1024, 512, 32]
      filters: [8, 16, 32]
      kernel_size: 7
      batchnorm: true
    enEr:
      cropping: [0, 0]
      dense_layers: [1024, 256, 64]
      filters: [8, 16, 32]
      kernel_size: 9
      batchnorm: true
    bl:
      cropping: [0, 0]
      dense_layers: [1024, 512, 64]
      filters: [16, 32, 64]
      kernel_size: 5
      batchnorm: true
    Vrf:
      cropping: [12, 12]
      dense_layers: [1024, 256, 64]
      filters: [8, 16, 32]
      kernel_size: 3
      batchnorm: true

# - encoder:
#     epochs: 200
#     dataset%: 1
#     cropping: 12
#     patch_size: [8, 8]
#     projection_dim: 8
#     transformer_layers: 4
#     num_heads: 12
#     activation: relu
#     transformer_units: [256, 16]
#     mlp_head_units: [256]
#     dropout_attention: 0.
#     dropout_mlp: 0.0
#     dropout_representation: 0.0
#     dropout_final: 0.0
#     learning_rate: 0.001
#     batch_size: 32
#     loss: 'mse'
#     use_bias: false
#     normalization: minmax
#     img_normalize: 'off'
#     ps_normalize: 'off'
#     final_activation: linear
#     optimizer: adam
#   model_cfg:
#     VrfSPS:
#       transformer_units: [1024, 64, 8]
#       transformer_layers: 4
#       mlp_head_units: [1024, 64, 8]
#       num_heads: 20


# - encoder:
#     epochs: 200
#     dataset%: 1
#     cropping: 12
#     patch_size: [8, 8]
#     projection_dim: 12
#     transformer_layers: 4
#     num_heads: 12
#     activation: relu
#     transformer_units: [256, 16]
#     mlp_head_units: [256]
#     dropout_attention: 0.
#     dropout_mlp: 0.0
#     dropout_representation: 0.0
#     dropout_final: 0.0
#     learning_rate: 0.001
#     batch_size: 32
#     loss: 'mse'
#     use_bias: false
#     normalization: minmax
#     img_normalize: 'off'
#     ps_normalize: 'off'
#     final_activation: linear
#     optimizer: adam
#   model_cfg:
#     VrfSPS:
#       transformer_units: [1024, 64, 12]
#       transformer_layers: 4
#       mlp_head_units: [1024, 64, 12]
#       num_heads: 20

# - encoder:
#     epochs: 200
#     dataset%: 1
#     cropping: 12
#     patch_size: [8, 8]
#     projection_dim: 16
#     transformer_layers: 4
#     num_heads: 12
#     activation: relu
#     transformer_units: [256, 16]
#     mlp_head_units: [256]
#     dropout_attention: 0.
#     dropout_mlp: 0.0
#     dropout_representation: 0.0
#     dropout_final: 0.0
#     learning_rate: 0.001
#     batch_size: 32
#     loss: 'mse'
#     use_bias: false
#     normalization: minmax
#     img_normalize: 'off'
#     ps_normalize: 'off'
#     final_activation: linear
#     optimizer: adam
#   model_cfg:
#     VrfSPS:
#       transformer_units: [1024, 64, 16]
#       transformer_layers: 4
#       mlp_head_units: [1024, 64, 16]
#       num_heads: 20


# - encoder:
#     epochs: 200
#     dataset%: 1
#     cropping: 12
#     patch_size: [8, 8]
#     projection_dim: 20
#     transformer_layers: 4
#     num_heads: 12
#     activation: relu
#     transformer_units: [256, 16]
#     mlp_head_units: [256]
#     dropout_attention: 0.
#     dropout_mlp: 0.0
#     dropout_representation: 0.0
#     dropout_final: 0.0
#     learning_rate: 0.001
#     batch_size: 32
#     loss: 'mse'
#     use_bias: false
#     normalization: minmax
#     img_normalize: 'off'
#     ps_normalize: 'off'
#     final_activation: linear
#     optimizer: adam
#   model_cfg:
#     VrfSPS:
#       transformer_units: [1024, 256, 20]
#       transformer_layers: 4
#       mlp_head_units: [1024, 64, 20]
#       num_heads: 20


# - encoder:
#     epochs: 200
#     dataset%: 1
#     cropping: 12
#     patch_size: [8, 8]
#     projection_dim: 24
#     transformer_layers: 4
#     num_heads: 12
#     activation: relu
#     transformer_units: [256, 16]
#     mlp_head_units: [256]
#     dropout_attention: 0.
#     dropout_mlp: 0.0
#     dropout_representation: 0.0
#     dropout_final: 0.0
#     learning_rate: 0.001
#     batch_size: 32
#     loss: 'mse'
#     use_bias: false
#     normalization: minmax
#     img_normalize: 'off'
#     ps_normalize: 'off'
#     final_activation: linear
#     optimizer: adam
#   model_cfg:
#     VrfSPS:
#       transformer_units: [1024, 256, 24]
#       transformer_layers: 4
#       mlp_head_units: [1024, 64, 24]
#       num_heads: 20

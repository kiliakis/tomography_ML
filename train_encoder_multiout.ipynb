{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the ML model\n",
    "from mlp_lhc_tomography.models import EncoderSingle, EncoderMulti\n",
    "from mlp_lhc_tomography.utils import sample_files, load_encoder_data\n",
    "from mlp_lhc_tomography.utils import encoder_files_to_tensors, fast_tensor_load\n",
    "from local_utils import plot_loss\n",
    "\n",
    "import time\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import yaml\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_Turns_Case = 1\n",
    "var_names = ['phEr', 'enEr', 'bl',\n",
    "             'inten', 'Vrf', 'mu', 'VrfSPS']\n",
    "# Initialize parameters\n",
    "# data_dir = '/eos/user/k/kiliakis/tomo_data/datasets_encoder_02-12-22'\n",
    "data_dir = './tomo_data/datasets_encoder_02-12-22'\n",
    "\n",
    "# data_dir = './tomo_data/datasets'\n",
    "timestamp = datetime.now().strftime(\"%Y_%m_%d_%H-%M-%S\")\n",
    "print('Using timestamp: ', timestamp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data specific\n",
    "IMG_OUTPUT_SIZE = 128\n",
    "BATCH_SIZE = 32  # 8\n",
    "\n",
    "# Train specific\n",
    "train_cfg = {\n",
    "    'epochs': 10,\n",
    "    'dense_layers': [64],\n",
    "    'filters': [8, 16],\n",
    "    'cropping': [0, 0],\n",
    "    'kernel_size': 7,\n",
    "    'strides': [2, 2],\n",
    "    'activation': 'relu',\n",
    "    'pooling': None,\n",
    "    'pooling_size': [0, 0],\n",
    "    'pooling_strides': [1, 1],\n",
    "    'pooling_padding': 'valid',\n",
    "    'dropout': 0.1,\n",
    "    'loss': 'mse',\n",
    "    'lr': 1e-3,\n",
    "    'dataset%': 1,\n",
    "    'normalization': 'minmax',\n",
    "    'loss_weights': [0, 1, 2, 3, 4, 5, 6]\n",
    "\n",
    "}\n",
    "\n",
    "model_cfg = {\n",
    "    # Best phEr config --> 2.79e-5 val_loss\n",
    "    'phEr': {\n",
    "        'epochs': 30,\n",
    "        'cropping': [0, 0],\n",
    "        'filters': [4, 8],\n",
    "        'kernel_size': [3, 3],\n",
    "        'strides': [2, 2],\n",
    "        'dense_layers': [1024, 256, 32],\n",
    "        'activation': 'relu',\n",
    "        'pooling': None,\n",
    "        'dropout': 0.,\n",
    "        'lr': 1e-3,\n",
    "        'normalization': 'minmax',\n",
    "        'batch_size': 32\n",
    "    },\n",
    "    # Best enErr config --> 6.71e-05 val_loss\n",
    "    'enEr': {\n",
    "        'epochs': 30,\n",
    "        'cropping': [12, 12],\n",
    "        'filters': [8, 16, 32],\n",
    "        'kernel_size': [3, 3, 3],\n",
    "        'strides': [2, 2],\n",
    "        'dense_layers': [1024, 256, 64],\n",
    "        'activation': 'relu',\n",
    "        'pooling': None,\n",
    "        'dropout': 0.,\n",
    "        'lr': 1e-3,\n",
    "        'normalization': 'minmax',\n",
    "        'batch_size': 32\n",
    "    },\n",
    "    # Best bl config --> 2.43e-04 val_loss\n",
    "    'bl': {\n",
    "        'epochs': 30,\n",
    "        'cropping': [12, 12],\n",
    "        'filters': [8, 16, 32],\n",
    "        'kernel_size': [(13, 3), (7, 3), (3, 3)],\n",
    "        'strides': [2, 2],\n",
    "        'dense_layers': [1024, 256, 64],\n",
    "        'activation': 'relu',\n",
    "        'pooling': None,\n",
    "        'dropout': 0.,\n",
    "        'lr': 5e-4,\n",
    "        'normalization': 'minmax',\n",
    "        'batch_size': 32\n",
    "    },\n",
    "    # Best inten config --> 7.82e-02 val_loss\n",
    "    'inten': {\n",
    "        'epochs': 30,\n",
    "        'cropping': [12, 12],\n",
    "        'filters': [8, 16, 32],\n",
    "        'kernel_size': [13, 7, 3],\n",
    "        'strides': [2, 2],\n",
    "        'dense_layers': [1024, 256, 64],\n",
    "        'activation': 'relu',\n",
    "        'pooling': None,\n",
    "        'dropout': 0.,\n",
    "        'lr': 1e-3,\n",
    "        'normalization': 'minmax',\n",
    "        'batch_size': 32\n",
    "    },\n",
    "    'mu': {\n",
    "        'epochs': 100,\n",
    "        'cropping': [0, 0],\n",
    "        'filters': [8, 16, 32],\n",
    "        'kernel_size': [5, 5, 5],\n",
    "        'strides': [2, 2],\n",
    "        'dense_layers': [1024, 256, 64],\n",
    "        'activation': 'relu',\n",
    "        'pooling': None,\n",
    "        'dropout': 0.,\n",
    "        'lr': 1e-3,\n",
    "        'normalization': 'minmax',\n",
    "        'batch_size': 32\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize directories\n",
    "trial_dir = os.path.join('./trials/', timestamp)\n",
    "weights_dir = os.path.join(trial_dir, 'weights')\n",
    "plots_dir = os.path.join(trial_dir, 'plots')\n",
    "\n",
    "# Initialize train/ test / validation paths\n",
    "ML_dir = os.path.join(data_dir, 'ML_data')\n",
    "TRAINING_PATH = os.path.join(ML_dir, 'TRAINING')\n",
    "VALIDATION_PATH = os.path.join(ML_dir, 'VALIDATION')\n",
    "assert os.path.exists(TRAINING_PATH)\n",
    "assert os.path.exists(VALIDATION_PATH)\n",
    "\n",
    "# create the directory to store the results\n",
    "os.makedirs(trial_dir, exist_ok=True)\n",
    "os.makedirs(weights_dir, exist_ok=False)\n",
    "os.makedirs(plots_dir, exist_ok=False)\n",
    "\n",
    "# Initialize GPU\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "device_to_use = 0\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        tf.config.experimental.set_memory_growth(gpus[device_to_use], True)\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[device_to_use],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=12*1024)])\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(\n",
    "            logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print('No GPU available, using the CPU')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_t = time.time()\n",
    "# Create the datasets\n",
    "# 1. Randomly select the training data\n",
    "file_names = sample_files(TRAINING_PATH, train_cfg['dataset%'], keep_every=num_Turns_Case)\n",
    "print('Number of Training files: ', len(file_names))\n",
    "\n",
    "x_train, y_train = encoder_files_to_tensors(\n",
    "    file_names, normalization=train_cfg['normalization'])\n",
    "\n",
    "# Repeat for validation data\n",
    "file_names = sample_files(\n",
    "    VALIDATION_PATH, train_cfg['dataset%'], keep_every=num_Turns_Case)\n",
    "print('Number of Validation files: ', len(file_names))\n",
    "\n",
    "x_valid, y_valid = encoder_files_to_tensors(\n",
    "    file_names, normalization=train_cfg['normalization'])\n",
    "\n",
    "end_t = time.time()\n",
    "print(\n",
    "    f'\\n---- Input files have been read, elapsed: {end_t - start_t} ----\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_t = time.time()\n",
    "# Create the datasets\n",
    "# 1. Randomly select the training data\n",
    "file_names = sample_files(TRAINING_PATH, train_cfg['dataset%'], keep_every=num_Turns_Case)\n",
    "print('Number of Training files: ', len(file_names))\n",
    "\n",
    "# 2. Convert to tensor dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(file_names)\n",
    "\n",
    "# 3. Then map function to dataset\n",
    "# this returns pairs of tensors with shape (128, 128, 1) and (7,)\n",
    "train_dataset = train_dataset.map(lambda x: tf.py_function(\n",
    "    load_encoder_data,\n",
    "    [x, train_cfg['normalization'], True],\n",
    "    [tf.float32, tf.float32]))\n",
    "\n",
    "# 4. Ignore errors in case they appear\n",
    "train_dataset = train_dataset.apply(\n",
    "    tf.data.experimental.ignore_errors())\n",
    "\n",
    "# Repeat for validation data\n",
    "file_names = sample_files(\n",
    "    VALIDATION_PATH, train_cfg['dataset%'], keep_every=num_Turns_Case)\n",
    "print('Number of Validation files: ', len(file_names))\n",
    "\n",
    "# convert to dataset\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices(file_names)\n",
    "# Then map function to dataset\n",
    "# this returns pairs of tensors with shape (128, 128, 1) and (7,)\n",
    "valid_dataset = valid_dataset.map(lambda x: tf.py_function(\n",
    "    load_encoder_data,\n",
    "    [x, train_cfg['normalization'], True],\n",
    "    [tf.float32, tf.float32]))\n",
    "# Ignore errors\n",
    "valid_dataset = valid_dataset.apply(\n",
    "    tf.data.experimental.ignore_errors())\n",
    "\n",
    "end_t = time.time()\n",
    "print(\n",
    "    f'\\n---- Input files have been read, elapsed: {end_t - start_t} ----\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# plot some of the outputs\n",
    "\n",
    "nrows = 3\n",
    "# Get nrows * nrows random images\n",
    "sample = np.random.choice(np.arange(len(x_train)),\n",
    "                          size=nrows * nrows, replace=False)\n",
    "\n",
    "samples_X = tf.gather(x_train, sample)\n",
    "samples_y = tf.gather(y_train, sample)\n",
    "\n",
    "# Create 3x3 grid of figures\n",
    "fig, axes = plt.subplots(ncols=nrows, nrows=nrows, figsize=(12, 12))\n",
    "axes = np.ravel(axes)\n",
    "for i in range(len(axes)):\n",
    "    ax = axes[i]\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    # show the image\n",
    "    ax.imshow(samples_X[i], cmap='jet')\n",
    "    # Set the label\n",
    "    title = ','.join([f'{num:.1f}' for num in samples_y[i]])\n",
    "    ax.set_title(f'{title}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train specific\n",
    "train_cfg = {\n",
    "    'epochs': 10,\n",
    "    'dense_layers': [16],\n",
    "    'filters': [8],\n",
    "    'cropping': [0, 0],\n",
    "    'kernel_size': 7,\n",
    "    'strides': [2, 2],\n",
    "    'activation': 'relu',\n",
    "    'pooling': None,\n",
    "    'pooling_size': [0, 0],\n",
    "    'pooling_strides': [1, 1],\n",
    "    'pooling_padding': 'valid',\n",
    "    'dropout': 0.1,\n",
    "    'loss': 'mse',\n",
    "    'lr': 1e-3,\n",
    "    'dataset%': 1,\n",
    "    'normalization': 'minmax',\n",
    "    'loss_weights': [2],\n",
    "    'batch_size': 32\n",
    "}\n",
    "\n",
    "model_cfg = {\n",
    "    'bl': {\n",
    "        'epochs': 100,\n",
    "        'cropping': [12, 12],\n",
    "        'filters': [8, 16, 32],\n",
    "        'kernel_size': [(13, 3), (7, 3), (3, 3)],\n",
    "        'strides': [2, 2],\n",
    "        'dense_layers': [1024, 512, 128],\n",
    "        'activation': 'relu',\n",
    "        'pooling': None,\n",
    "        'dropout': 0.,\n",
    "        'lr': 1e-3,\n",
    "        'normalization': 'minmax',\n",
    "        'batch_size': 32\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "output_names = np.array(var_names)[train_cfg['loss_weights']]\n",
    "print(output_names)\n",
    "\n",
    "# Model instantiation\n",
    "start_t = time.time()\n",
    "input_shape = (IMG_OUTPUT_SIZE, IMG_OUTPUT_SIZE, 1)\n",
    "models = {}\n",
    "\n",
    "for i in train_cfg['loss_weights']:\n",
    "    var_name = var_names[i]\n",
    "    print(f'\\n---- Initializing model: {var_name} ----\\n')\n",
    "\n",
    "    cfg = train_cfg.copy()\n",
    "    cfg.update(model_cfg.get(var_name, {}))\n",
    "    model_cfg[var_name] = cfg\n",
    "    \n",
    "    model = EncoderSingle(output_name=var_name,\n",
    "                          input_shape=input_shape,\n",
    "                         **cfg)\n",
    "    print(model.model.summary())\n",
    "\n",
    "    models[var_name] = {'model': model.model,\n",
    "                        'train': tf.gather(y_train, i, axis=1),\n",
    "                        'valid': tf.gather(y_valid, i, axis=1)}\n",
    "print(\n",
    "    f'\\n---- Models have been initialized, elapsed: {time.time() - start_t} ----\\n')\n",
    "\n",
    "\n",
    "# Train the encoder\n",
    "historyMulti = {}\n",
    "\n",
    "for var_name in models:\n",
    "    model = models[var_name]['model']\n",
    "    cfg = model_cfg[var_name]\n",
    "    # Train the encoder\n",
    "    print(f'\\n---- {var_name}: Training the encoder ----\\n')\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # callbacks, save the best model, and early stop if no improvement in val_loss\n",
    "    stop_early = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                               patience=10, restore_best_weights=True)\n",
    "    save_best = keras.callbacks.ModelCheckpoint(filepath=os.path.join(weights_dir, f'encoder_{var_name}.h5'),\n",
    "                                                monitor='val_loss', save_best_only=True)\n",
    "\n",
    "    history = model.fit(\n",
    "        x=x_train, y=models[var_name]['train'], \n",
    "        epochs=cfg['epochs'],\n",
    "        validation_data=(x_valid, models[var_name]['valid']),\n",
    "        callbacks=[save_best], \n",
    "        batch_size=cfg['batch_size'],\n",
    "        verbose=0)\n",
    "    historyMulti[f'{var_name}_loss'] = history.history['loss']\n",
    "    historyMulti[f'{var_name}_val_loss'] = history.history['val_loss']\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(\n",
    "        f'\\n---- {var_name}: Training complete, epochs: {len(history.history[\"loss\"])}, train loss {np.min(history.history[\"loss\"]):.2e}, valid loss {np.min(history.history[\"val_loss\"]):.2e}, total time {total_time} ----\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "train_loss_l = []\n",
    "valid_loss_l = []\n",
    "for k, v in historyMulti.items():\n",
    "    if 'val' in k:\n",
    "        valid_loss_l.append(v)\n",
    "    else:\n",
    "        train_loss_l.append(v)\n",
    "\n",
    "train_loss_l = np.mean(train_loss_l, axis=0)\n",
    "valid_loss_l = np.mean(valid_loss_l, axis=0)\n",
    "print(train_loss_l)\n",
    "plot_loss({'training': train_loss_l, 'validation': valid_loss_l},\n",
    "          title='Encoder Train/Validation Loss')\n",
    "\n",
    "plot_loss(historyMulti, title='Encoder loss per output')\n",
    "\n",
    "print(historyMulti)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model instantiation\n",
    "input_shape = (IMG_OUTPUT_SIZE, IMG_OUTPUT_SIZE, 1)\n",
    "train_cfg['loss_weights'] = [0, 1, 2, 3, 4, 5, 6]\n",
    "\n",
    "output_names = np.array(var_names)[train_cfg['loss_weights']]\n",
    "print(output_names)\n",
    "\n",
    "encoderMulti = EncoderMulti(input_shape=input_shape,\n",
    "                            output_names=output_names, **train_cfg)\n",
    "models = {}\n",
    "for i in train_cfg['loss_weights']:\n",
    "    var_name = var_names[i]\n",
    "    models[var_name] = {'model': encoderMulti.models[var_name],\n",
    "                        'train': train_dataset.map(lambda x, y: (x, y[i])).batch(1),\n",
    "                        'valid': valid_dataset.map(lambda x, y: (x, y[i])).batch(1)}\n",
    "    print(models[var_name]['model'].summary())\n",
    "\n",
    "# Train the encoder\n",
    "historyMulti = {}\n",
    "\n",
    "for var_name in models:\n",
    "    model = models[var_name]['model']\n",
    "\n",
    "    # Train the encoder\n",
    "\n",
    "    print(f'\\n---- {var_name}: Training the encoder ----\\n')\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # callbacks, save the best model, and early stop if no improvement in val_loss\n",
    "    stop_early = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                               patience=5, restore_best_weights=True)\n",
    "    save_best = keras.callbacks.ModelCheckpoint(filepath=os.path.join(weights_dir, f'encoder_{var_name}.h5'),\n",
    "                                                monitor='val_loss', save_best_only=True)\n",
    "\n",
    "    history = model.fit(\n",
    "        models[var_name]['train'], epochs=train_cfg['epochs'],\n",
    "        validation_data=models[var_name]['valid'],\n",
    "        callbacks=[stop_early, save_best],\n",
    "        verbose=0)\n",
    "    historyMulti[f'{var_name}_loss'] = history.history['loss']\n",
    "    historyMulti[f'{var_name}_val_loss'] = history.history['val_loss']\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(\n",
    "        f'\\n---- {var_name}: Training complete, epochs: {len(history.history[\"loss\"])}, min loss {np.min(history.history[\"val_loss\"])}, total time {total_time} ----\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "train_loss_l = []\n",
    "valid_loss_l = []\n",
    "for k, v in historyMulti.items():\n",
    "    if 'val' in k:\n",
    "        valid_loss_l.append(v)\n",
    "    else:\n",
    "        train_loss_l.append(v)\n",
    "\n",
    "train_loss_l = np.mean(train_loss_l, axis=0)\n",
    "valid_loss_l = np.mean(valid_loss_l, axis=0)\n",
    "print(train_loss_l)\n",
    "plot_loss({'training': train_loss_l, 'validation': valid_loss_l},\n",
    "            title='Encoder Train/Validation Loss')\n",
    "\n",
    "plot_loss(historyMulti, title='Encoder loss per output')\n",
    "\n",
    "print(historyMulti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Encoder\n",
    "# Model instantiation\n",
    "\n",
    "input_shape = (IMG_OUTPUT_SIZE, IMG_OUTPUT_SIZE, 1)\n",
    "\n",
    "encoder = Encoder(input_shape=input_shape, **train_cfg)\n",
    "\n",
    "print(encoder.model.summary())\n",
    "\n",
    "# Train the encoder\n",
    "\n",
    "# callbacks, save the best model, and early stop if no improvement in val_loss\n",
    "stop_early = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                           patience=5, restore_best_weights=True)\n",
    "save_best = keras.callbacks.ModelCheckpoint(filepath=os.path.join(weights_dir, 'encoder.h5'),\n",
    "                                            monitor='val_loss', save_best_only=True)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "# history = encoder.model.fit(\n",
    "#     train_dataset, epochs=train_cfg['epochs'],\n",
    "#     validation_data=valid_dataset,\n",
    "#     callbacks=[stop_early, save_best])\n",
    "history = encoder.model.fit(\n",
    "    x_train, y_train, epochs=train_cfg['epochs'],\n",
    "    validation_data=(x_valid, y_valid),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[stop_early, save_best])\n",
    "\n",
    "total_time = time.time() - start_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history)\n",
    "\n",
    "print(historyFunc.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = []\n",
    "val_total = []\n",
    "for k, v in history.history.items():\n",
    "    if k != 'loss' and 'val' not in k:\n",
    "        total.append(v)\n",
    "    if k != 'val_loss' and 'val' in k:\n",
    "        val_total.append(v)\n",
    "    \n",
    "total = np.array(total)\n",
    "val_total = np.array(val_total)\n",
    "\n",
    "weights = np.array(train_cfg['loss_weights']).reshape(-1, 1)\n",
    "print('Mean:', np.sum(total*weights, axis=0)/np.sum(weights))\n",
    "history.history['loss'] = np.sum(total*weights, axis=0)/np.sum(weights)\n",
    "history.history['val_loss'] = np.sum(val_total*weights, axis=0)/np.sum(weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "def plot_loss(lines, title='', figname=None):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    for line in lines.keys():\n",
    "        if 'val' in line:\n",
    "            marker='x'\n",
    "        else:\n",
    "            marker='.'\n",
    "        plt.semilogy(lines[line], marker=marker, label=line)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(ncol=2)\n",
    "    plt.tight_layout()\n",
    "    # if figname:\n",
    "    #     plt.savefig(figname, dpi=300)\n",
    "    # plt.close()\n",
    "\n",
    "plot_loss(history.history)\n",
    "\n",
    "train_loss_l = np.array(history.history['loss'])\n",
    "valid_loss_l = np.array(history.history['val_loss'])\n",
    "\n",
    "plot_loss({'Training': train_loss_l, 'Validation': valid_loss_l},\n",
    "          title='Encoder Train/Validation Loss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def my_mse(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.math.abs(y_pred - y_true), axis=0)\n",
    "\n",
    "\n",
    "y_true = np.array([[0., 0.], [1., 1.]])\n",
    "y_pred = np.array([[1., 0.5], [2., 1.5]])\n",
    "loss = keras.losses.mean_absolute_error(y_true, y_pred)\n",
    "mae2 = MyMeanAbsoluteError(reduction=keras.losses.Reduction.NONE)\n",
    "mae = keras.losses.MeanAbsoluteError(reduction=keras.losses.Reduction.NONE)\n",
    "lossv2 = mean_absolute_error(y_true, y_pred, multioutput='raw_values')\n",
    "# lossv3 = my_mse(1, y_true, y_pred)\n",
    "# print(loss)\n",
    "# print(mae(y_true, y_pred).numpy())\n",
    "print(lossv2)\n",
    "# print(mae2(y_true, y_pred).numpy())\n",
    "# lossv4 = my_mse(y_true, y_pred)\n",
    "\n",
    "print(my_mse(y_true, y_pred))\n",
    "# print(my_mse(1, y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss\n",
    "train_loss_l = np.array(history.history['loss'])\n",
    "valid_loss_l = np.array(history.history['val_loss'])\n",
    "\n",
    "plot_loss({'Training': train_loss_l, 'Validation': valid_loss_l},\n",
    "          title='Encoder Train/Validation Loss',\n",
    "          figname=os.path.join(plots_dir, 'encoder_train_valid_loss.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# get predictions\n",
    "y_pred = encoder.model.predict(x_valid, verbose=False)\n",
    "y_pred = np.array(y_pred)\n",
    "y_valid = np.array(y_valid)\n",
    "\n",
    "# Calculate error per variable\n",
    "mses = mean_squared_error(y_valid, y_pred, multioutput='raw_values')\n",
    "\n",
    "var_names = ['phase_error', 'energy_error',\n",
    "             'bunch_length', 'intensity', 'Volt_rf', 'mu', 'Vrf_SPS']\n",
    "# report\n",
    "print('Variable\\tMSE')\n",
    "for name, mse in zip(var_names, mses):\n",
    "    print(f'{name}:\\t{mse:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# get predictions\n",
    "y_pred = encoderFunc.model.predict(x_valid, verbose=False)\n",
    "y_pred = np.array(y_pred).reshape(7, -1).T\n",
    "y_valid = np.array(y_valid)\n",
    "# Calculate error per variable\n",
    "mses = mean_squared_error(y_valid, y_pred, multioutput='raw_values')\n",
    "print(mses.shape)\n",
    "var_names = ['phase_error', 'energy_error',\n",
    "             'bunch_length', 'intensity', 'Volt_rf', 'mu', 'Vrf_SPS']\n",
    "# report\n",
    "print('Variable\\tMSE')\n",
    "for name, mse in zip(var_names, mses):\n",
    "    print(f'{name}:\\t{mse:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save file with experiment configuration\n",
    "config_dict = {}\n",
    "config_dict['encoder'] = train_cfg.copy()\n",
    "config_dict['encoder'].update({\n",
    "    'min_train_loss': float(np.min(train_loss_l)),\n",
    "    'min_valid_loss': float(np.min(valid_loss_l)),\n",
    "    'total_train_time': total_time,\n",
    "    'used_gpus': len(gpus)\n",
    "})\n",
    "\n",
    "# save config_dict\n",
    "with open(os.path.join(trial_dir, 'encoder-summary.yml'), 'w') as configfile:\n",
    "    yaml.dump(config_dict, configfile, default_flow_style=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:26:10) \n[GCC 11.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "79ab8fd01a8cec42884b8b2a5d7fb4751c5402d97e9e61d151ed5c6a6352873c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
